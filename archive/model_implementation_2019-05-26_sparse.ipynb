{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joashc/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import LearningRateScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code copied from https://www.kaggle.com/marknagelberg/rmsle-function\n",
    "def rmsle(y_pred, y_test) : \n",
    "    assert len(y_test) == len(y_pred)\n",
    "    return np.sqrt(np.mean((np.log(1+y_pred) - np.log(1+y_test))**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets the count of most frequent words give a dataframe\n",
    "def word_freq(df, col):\n",
    "    word_frequency = {}\n",
    "    word_frequency_lst = []\n",
    "    for index,row in df.iterrows(): \n",
    "        for w in list(set(str(row[col]).split(' '))):\n",
    "            if w not in word_frequency:\n",
    "                word_frequency[w] = 1\n",
    "            else:\n",
    "                word_frequency[w] += 1\n",
    "\n",
    "    for key, value in word_frequency.items():\n",
    "        temp = [key, value]\n",
    "        word_frequency_lst.append(temp)\n",
    "    word_freq_df = pd.DataFrame(word_frequency_lst, columns=[\"unique_word\", 'frequency'])\n",
    "    word_freq_df = word_freq_df.sort_values(['frequency'], ascending=False)\n",
    "    return word_freq_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1482486, 13)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_data = pd.read_csv(\n",
    "    '/Users/joashc/Downloads/mercari-price-suggestion-challenge/partially_clean_train_data.csv')\n",
    "clean_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling Text\n",
    "- stemmed_item_description\n",
    "    - tdidf matrix\n",
    "- clean_brand_name\n",
    "    - One-hot-encode\n",
    "- clean_category_name\n",
    "    - One-hot-encode unique values if possible\n",
    "- clean_item_name\n",
    "    - tdidf matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF item_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1482486,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_description_df = clean_data['stemmed_item_description']\n",
    "item_description_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_item_desc_features = 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1482486, 1500)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_desc_0</th>\n",
       "      <th>item_desc_1</th>\n",
       "      <th>item_desc_2</th>\n",
       "      <th>item_desc_3</th>\n",
       "      <th>item_desc_4</th>\n",
       "      <th>item_desc_5</th>\n",
       "      <th>item_desc_6</th>\n",
       "      <th>item_desc_7</th>\n",
       "      <th>item_desc_8</th>\n",
       "      <th>item_desc_9</th>\n",
       "      <th>...</th>\n",
       "      <th>item_desc_1490</th>\n",
       "      <th>item_desc_1491</th>\n",
       "      <th>item_desc_1492</th>\n",
       "      <th>item_desc_1493</th>\n",
       "      <th>item_desc_1494</th>\n",
       "      <th>item_desc_1495</th>\n",
       "      <th>item_desc_1496</th>\n",
       "      <th>item_desc_1497</th>\n",
       "      <th>item_desc_1498</th>\n",
       "      <th>item_desc_1499</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.714017</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 1500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   item_desc_0  item_desc_1  item_desc_2  item_desc_3  item_desc_4  \\\n",
       "0          0.0          0.0          0.0          0.0          0.0   \n",
       "1          0.0          0.0          0.0          0.0          0.0   \n",
       "\n",
       "   item_desc_5  item_desc_6  item_desc_7  item_desc_8  item_desc_9  ...  \\\n",
       "0          0.0          0.0          0.0          0.0          0.0  ...   \n",
       "1          0.0          0.0          0.0          0.0          0.0  ...   \n",
       "\n",
       "   item_desc_1490  item_desc_1491  item_desc_1492  item_desc_1493  \\\n",
       "0        0.714017             0.0             0.0             0.0   \n",
       "1        0.000000             0.0             0.0             0.0   \n",
       "\n",
       "   item_desc_1494  item_desc_1495  item_desc_1496  item_desc_1497  \\\n",
       "0             0.0             0.0             0.0             0.0   \n",
       "1             0.0             0.0             0.0             0.0   \n",
       "\n",
       "   item_desc_1498  item_desc_1499  \n",
       "0             0.0             0.0  \n",
       "1             0.0             0.0  \n",
       "\n",
       "[2 rows x 1500 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(max_features=max_item_desc_features)\n",
    "x_tfidf = pd.DataFrame(tfidf.fit_transform(item_description_df).toarray())\n",
    "x_tfidf.columns = ['item_desc_' + str(col) for col in x_tfidf.columns]\n",
    "print(x_tfidf.shape)\n",
    "x_tfidf.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1482486, 13)\n",
      "(1482486, 1511)\n",
      "CPU times: user 1min 15s, sys: 1min 26s, total: 2min 41s\n",
      "Wall time: 3min 23s\n"
     ]
    }
   ],
   "source": [
    "print(clean_data.shape)\n",
    "clean_data_v2 = pd.concat([clean_data, x_tfidf], axis=1).drop(columns=['item_description', \n",
    "                                                                                         'stemmed_item_description'])\n",
    "print(clean_data_v2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete dataframes from memory\n",
    "del [[x_tfidf,item_description_df, clean_data]]\n",
    "gc.collect()\n",
    "clean_data = pd.DataFrame()\n",
    "item_description_df=pd.DataFrame()\n",
    "x_tfidf=pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF clean_item_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1482486,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# item_name_df = clean_data_v2['clean_item_name']\n",
    "# item_name_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_item_name_features = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1482486, 100)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_name_0</th>\n",
       "      <th>item_name_1</th>\n",
       "      <th>item_name_2</th>\n",
       "      <th>item_name_3</th>\n",
       "      <th>item_name_4</th>\n",
       "      <th>item_name_5</th>\n",
       "      <th>item_name_6</th>\n",
       "      <th>item_name_7</th>\n",
       "      <th>item_name_8</th>\n",
       "      <th>item_name_9</th>\n",
       "      <th>...</th>\n",
       "      <th>item_name_90</th>\n",
       "      <th>item_name_91</th>\n",
       "      <th>item_name_92</th>\n",
       "      <th>item_name_93</th>\n",
       "      <th>item_name_94</th>\n",
       "      <th>item_name_95</th>\n",
       "      <th>item_name_96</th>\n",
       "      <th>item_name_97</th>\n",
       "      <th>item_name_98</th>\n",
       "      <th>item_name_99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.554055</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   item_name_0  item_name_1  item_name_2  item_name_3  item_name_4  \\\n",
       "0          0.0          0.0          0.0          0.0          0.0   \n",
       "1          0.0          0.0          0.0          0.0          0.0   \n",
       "\n",
       "   item_name_5  item_name_6  item_name_7  item_name_8  item_name_9  ...  \\\n",
       "0          0.0          0.0          0.0          0.0          0.0  ...   \n",
       "1          0.0          0.0          0.0          0.0          0.0  ...   \n",
       "\n",
       "   item_name_90  item_name_91  item_name_92  item_name_93  item_name_94  \\\n",
       "0           0.0           0.0           0.0           0.0           0.0   \n",
       "1           0.0           0.0           0.0           0.0           0.0   \n",
       "\n",
       "   item_name_95  item_name_96  item_name_97  item_name_98  item_name_99  \n",
       "0           0.0           0.0           0.0      0.554055           0.0  \n",
       "1           0.0           0.0           0.0      0.000000           0.0  \n",
       "\n",
       "[2 rows x 100 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tfidf = TfidfVectorizer(max_features=max_item_name_features)\n",
    "# item_name_tfidf = pd.DataFrame(tfidf.fit_transform(item_name_df).toarray())\n",
    "# item_name_tfidf.columns = ['item_name_' + str(col) for col in item_name_tfidf.columns]\n",
    "# print(item_name_tfidf.shape)\n",
    "# item_name_tfidf.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1482486, 1837)\n",
      "(1482486, 1935)\n",
      "CPU times: user 41.3 s, sys: 1min 17s, total: 1min 58s\n",
      "Wall time: 2min 24s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# print(clean_data_v4.shape)\n",
    "# clean_data_v5 = pd.concat([clean_data_v4, item_name_tfidf], axis=1).drop(columns=['clean_item_name', \n",
    "#                                                                                          'name'])\n",
    "print(clean_data_v5.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Delete dataframes from memory\n",
    "# del [[item_name_tfidf,item_name_df, clean_data_v4]]\n",
    "# gc.collect()\n",
    "# clean_data_v4 = pd.DataFrame()\n",
    "# item_name_df=pd.DataFrame()\n",
    "# item_name_tfidf=pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Unwanted Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1482486, 1503)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_data_v2 = clean_data_v2.drop(columns=list(clean_data_v2.select_dtypes(object)))\n",
    "clean_data_v2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1482486, 1501)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_data_v2 = clean_data_v2.drop(columns= ['item_condition_id','shipping'])\n",
    "clean_data_v2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in train and validation data: 1260113 (1260113, 1)\n",
      "Number of rows in test data: 222373 (222373, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(clean_data_v2.drop(columns=['price']).reset_index(drop=True), \n",
    "                                                    clean_data_v2[['price']].reset_index(drop=True), \n",
    "                                                                  test_size=0.15, random_state=42)\n",
    "print('Number of rows in train and validation data:', X_train.shape[0], y_train.shape)\n",
    "print('Number of rows in test data:', X_test.shape[0], y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# print('MinMaxScaler Complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model architecture parameters\n",
    "n_stocks = 500\n",
    "n_neurons_1 = 1024\n",
    "n_neurons_2 = 512\n",
    "n_neurons_3 = 256\n",
    "n_neurons_4 = 128\n",
    "n_target = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in train and validation data: 1071096 (1071096, 1)\n",
      "Number of rows in validation data: 189017 (189017, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train.reset_index(drop=True), \n",
    "                                                    y_train.reset_index(drop=True), \n",
    "                                                                  test_size=0.15, random_state=42)\n",
    "print('Number of rows in train and validation data:', X_train.shape[0], y_train.shape)\n",
    "print('Number of rows in validation data:', X_val.shape[0], y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.002\n",
    "    drop = 0.5\n",
    "    epochs_drop = 20\n",
    "    lrate = initial_lrate * math.pow(drop,\n",
    "    math.floor((1+epoch)/epochs_drop))\n",
    "    return lrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1071096 samples, validate on 189017 samples\n",
      "Epoch 1/15\n",
      " 367358/1071096 [=========>....................] - ETA: 14:07 - loss: 1884.5682 - mean_squared_error: 1884.5682"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-fbf3e616ec6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m model_hist = model.fit(X_train, y_train, validation_data=(X_val,y_val), \n\u001b[0;32m---> 32\u001b[0;31m                        batch_size=batch_size, epochs=num_epochs, verbose=1)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    183\u001b[0m                         \u001b[0;31m# Do not slice the training phase flag.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                         ins_batch = slice_arrays(\n\u001b[0;32m--> 185\u001b[0;31m                             ins[:-1], batch_ids) + [ins[-1]]\n\u001b[0m\u001b[1;32m    186\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                         \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mslice_arrays\u001b[0;34m(arrays, start, stop)\u001b[0m\n\u001b[1;32m    521\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 523\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    524\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    521\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 523\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    524\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 15\n",
    "batch_size = 2213\n",
    "\n",
    "all_val_predictions = pd.DataFrame()\n",
    "\n",
    "train_val_rmsle = []\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_dim=X_train.shape[1]))\n",
    "model.add(BatchNormalization())\n",
    "# model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(264, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "# model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "# model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "# model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(1))\n",
    "#     model.add(Activation('sigmoid'))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error'])\n",
    "\n",
    "lrate = LearningRateScheduler(step_decay)\n",
    "\n",
    "model_hist = model.fit(X_train, y_train, validation_data=(X_val,y_val), \n",
    "                       batch_size=batch_size, epochs=num_epochs, verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1024, activation='relu', input_dim=X_train.shape[1]))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(512, activation='relu', input_dim=X_train.shape[1]))\n",
    "model.add(BatchNormalization())\n",
    "# model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(264, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "# model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "# model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "# model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(1))\n",
    "\n",
    "Epoch 1/15\n",
    "2019-06-01 21:52:00.440682: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
    "1071096/1071096 [==============================] - 62s 58us/step - loss: 1575.1133 - mean_squared_logarithmic_error: 3.5816 - val_loss: 1127.1075 - val_mean_squared_logarithmic_error: 0.5566\n",
    "Epoch 2/15\n",
    "1071096/1071096 [==============================] - 58s 54us/step - loss: 1043.6532 - mean_squared_logarithmic_error: 0.4336 - val_loss: 1087.2679 - val_mean_squared_logarithmic_error: 0.4298\n",
    "Epoch 3/15\n",
    "1071096/1071096 [==============================] - 57s 54us/step - loss: 901.0541 - mean_squared_logarithmic_error: 0.4288 - val_loss: 1053.5793 - val_mean_squared_logarithmic_error: 0.4423\n",
    "Epoch 4/15\n",
    "1071096/1071096 [==============================] - 58s 54us/step - loss: 782.7322 - mean_squared_logarithmic_error: 0.4164 - val_loss: 1047.5116 - val_mean_squared_logarithmic_error: 0.4374\n",
    "Epoch 5/15\n",
    "1071096/1071096 [==============================] - 58s 54us/step - loss: 682.4031 - mean_squared_logarithmic_error: 0.4012 - val_loss: 1040.5188 - val_mean_squared_logarithmic_error: 0.4202\n",
    "Epoch 6/15\n",
    "1071096/1071096 [==============================] - 58s 54us/step - loss: 610.8303 - mean_squared_logarithmic_error: 0.3871 - val_loss: 1057.9986 - val_mean_squared_logarithmic_error: 0.4210\n",
    "Epoch 7/15\n",
    "1071096/1071096 [==============================] - 58s 54us/step - loss: 549.4215 - mean_squared_logarithmic_error: 0.3753 - val_loss: 1055.0224 - val_mean_squared_logarithmic_error: 0.4625\n",
    "Epoch 8/15\n",
    "1071096/1071096 [==============================] - 58s 54us/step - loss: 520.1334 - mean_squared_logarithmic_error: 0.3707 - val_loss: 1070.6939 - val_mean_squared_logarithmic_error: 0.4277\n",
    "Epoch 9/15\n",
    "1071096/1071096 [==============================] - 57s 54us/step - loss: 488.1673 - mean_squared_logarithmic_error: 0.3607 - val_loss: 1063.8773 - val_mean_squared_logarithmic_error: 0.4367\n",
    "Epoch 10/15\n",
    "1071096/1071096 [==============================] - 57s 54us/step - loss: 460.5009 - mean_squared_logarithmic_error: 0.3528 - val_loss: 1089.4883 - val_mean_squared_logarithmic_error: 0.4295\n",
    "Epoch 11/15\n",
    "1071096/1071096 [==============================] - 57s 54us/step - loss: 433.2903 - mean_squared_logarithmic_error: 0.3439 - val_loss: 1088.7680 - val_mean_squared_logarithmic_error: 0.4340\n",
    "Epoch 12/15\n",
    "1071096/1071096 [==============================] - 58s 54us/step - loss: 426.6830 - mean_squared_logarithmic_error: 0.3411 - val_loss: 1092.9512 - val_mean_squared_logarithmic_error: 0.4301\n",
    "Epoch 13/15\n",
    "1071096/1071096 [==============================] - 57s 54us/step - loss: 403.5846 - mean_squared_logarithmic_error: 0.3323 - val_loss: 1106.4169 - val_mean_squared_logarithmic_error: 0.4234\n",
    "Epoch 14/15\n",
    "1071096/1071096 [==============================] - 57s 54us/step - loss: 401.3351 - mean_squared_logarithmic_error: 0.3337 - val_loss: 1082.3636 - val_mean_squared_logarithmic_error: 0.4376\n",
    "Epoch 15/15\n",
    "1071096/1071096 [==============================] - 57s 54us/step - loss: 387.6622 - mean_squared_logarithmic_error: 0.3279 - val_loss: 1104.6679 - val_mean_squared_logarithmic_error: 0.4299\n",
    "{'val_loss': [1127.1074626043912, 1087.2678627735013, 1053.5792987709883, 1047.5116281083513, 1040.518787082659, 1057.9986405238083, 1055.022360304423, 1070.6938961554606, 1063.8773043713986, 1089.4883044629082, 1088.7680027868587, 1092.9512373824361, 1106.4168555640338, 1082.3635730870856, 1104.6678808366034], 'val_mean_squared_logarithmic_error': [0.5565659740499078, 0.4297962513817454, 0.4422843877662052, 0.43737004754706865, 0.420194410348369, 0.42095913532351426, 0.46250864383542617, 0.4277115894045907, 0.4366671346470959, 0.42945249548830833, 0.4339731699247617, 0.4301039080199287, 0.4233793237558755, 0.4375614984162814, 0.42992322702482433], 'loss': [1575.1133286284676, 1043.653200820877, 901.0541341271995, 782.7321535718904, 682.4030638488076, 610.8303089746302, 549.4214598600078, 520.1334156088888, 488.1673113337513, 460.5008624314031, 433.29030865372914, 426.68298521459775, 403.5845953510768, 401.33514786087136, 387.6621897323485], 'mean_squared_logarithmic_error': [3.5815836171978437, 0.4336171417652295, 0.4288192078297505, 0.41643600321520013, 0.40117916568333656, 0.3871199580829268, 0.3752621106127402, 0.370657310023544, 0.3606964625948292, 0.35277935667126714, 0.3438604436602765, 0.34114952042988345, 0.33234044059753115, 0.3336569687144336, 0.32790030767299455]}\n",
    "Train error is: 21.96011656311792\n",
    "Validation error is: 33.236544404140375\n",
    "\n",
    "    \n",
    "# num_epochs = 15\n",
    "# batch_size = 2213\n",
    "# Brand frequency: 100\n",
    "# lr: 0.002\n",
    "# test/train split: 0.15\n",
    "# test/val split: 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1024, activation='relu', input_dim=X_train.shape[1]))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(512, activation='relu', input_dim=X_train.shape[1]))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(264, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(1))\n",
    "\n",
    "Epoch 1/15\n",
    "2019-06-01 22:28:11.231741: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
    "1071096/1071096 [==============================] - 61s 57us/step - loss: 1572.8784 - mean_squared_logarithmic_error: 3.4729 - val_loss: 1217.6416 - val_mean_squared_logarithmic_error: 0.9015\n",
    "Epoch 2/15\n",
    "1071096/1071096 [==============================] - 57s 54us/step - loss: 1041.6045 - mean_squared_logarithmic_error: 0.4348 - val_loss: 1047.5824 - val_mean_squared_logarithmic_error: 0.4541\n",
    "Epoch 3/15\n",
    "1071096/1071096 [==============================] - 58s 54us/step - loss: 895.4890 - mean_squared_logarithmic_error: 0.4270 - val_loss: 1048.9667 - val_mean_squared_logarithmic_error: 0.4354\n",
    "Epoch 4/15\n",
    "1071096/1071096 [==============================] - 58s 54us/step - loss: 783.4882 - mean_squared_logarithmic_error: 0.4162 - val_loss: 1076.8767 - val_mean_squared_logarithmic_error: 0.4711\n",
    "Epoch 5/15\n",
    "1071096/1071096 [==============================] - 58s 54us/step - loss: 687.1998 - mean_squared_logarithmic_error: 0.4011 - val_loss: 1059.8252 - val_mean_squared_logarithmic_error: 0.4338\n",
    "Epoch 6/15\n",
    "1071096/1071096 [==============================] - 58s 54us/step - loss: 616.5953 - mean_squared_logarithmic_error: 0.3865 - val_loss: 1102.7695 - val_mean_squared_logarithmic_error: 0.4481\n",
    "Epoch 7/15\n",
    "1071096/1071096 [==============================] - 58s 54us/step - loss: 568.2803 - mean_squared_logarithmic_error: 0.3784 - val_loss: 1067.3536 - val_mean_squared_logarithmic_error: 0.4238\n",
    "Epoch 8/15\n",
    "1071096/1071096 [==============================] - 57s 54us/step - loss: 520.4558 - mean_squared_logarithmic_error: 0.3723 - val_loss: 1083.0141 - val_mean_squared_logarithmic_error: 0.4252\n",
    "Epoch 9/15\n",
    "1071096/1071096 [==============================] - 57s 54us/step - loss: 490.2047 - mean_squared_logarithmic_error: 0.3583 - val_loss: 1090.4227 - val_mean_squared_logarithmic_error: 0.4484\n",
    "Epoch 10/15\n",
    "1071096/1071096 [==============================] - 57s 54us/step - loss: 475.4367 - mean_squared_logarithmic_error: 0.3576 - val_loss: 1084.0384 - val_mean_squared_logarithmic_error: 0.4213\n",
    "Epoch 11/15\n",
    "1071096/1071096 [==============================] - 57s 54us/step - loss: 450.5099 - mean_squared_logarithmic_error: 0.3493 - val_loss: 1090.1185 - val_mean_squared_logarithmic_error: 0.4410\n",
    "Epoch 12/15\n",
    "1071096/1071096 [==============================] - 57s 54us/step - loss: 420.6035 - mean_squared_logarithmic_error: 0.3392 - val_loss: 1088.9401 - val_mean_squared_logarithmic_error: 0.4247\n",
    "Epoch 13/15\n",
    "1071096/1071096 [==============================] - 57s 54us/step - loss: 402.3568 - mean_squared_logarithmic_error: 0.3319 - val_loss: 1093.5605 - val_mean_squared_logarithmic_error: 0.4199\n",
    "Epoch 14/15\n",
    "1071096/1071096 [==============================] - 57s 53us/step - loss: 395.7446 - mean_squared_logarithmic_error: 0.3300 - val_loss: 1075.7294 - val_mean_squared_logarithmic_error: 0.4278\n",
    "Epoch 15/15\n",
    "1071096/1071096 [==============================] - 57s 54us/step - loss: 380.5325 - mean_squared_logarithmic_error: 0.3242 - val_loss: 1089.9974 - val_mean_squared_logarithmic_error: 0.4582\n",
    "{'val_loss': [1217.6416338833594, 1047.5823814230514, 1048.9667078853502, 1076.8766861056768, 1059.8251634824612, 1102.7695363280557, 1067.353624158997, 1083.0141257887979, 1090.4227442875224, 1084.03840107145, 1090.1185013321362, 1088.940090125215, 1093.5605033395357, 1075.7294467749698, 1089.9973991865206], 'val_mean_squared_logarithmic_error': [0.9015497393156829, 0.4540567453312572, 0.4353959632427432, 0.47106237034087345, 0.43382145470214695, 0.4481305697343656, 0.423842325748674, 0.4252363223204157, 0.44844919863147775, 0.42133685137869387, 0.44104321300297566, 0.4246970767350278, 0.4199395750548852, 0.4278126349446862, 0.4582459891594587], 'loss': [1572.8783934025064, 1041.604502517444, 895.4889809983421, 783.4881842577598, 687.1998442333631, 616.595347463793, 568.280334914395, 520.4557500252855, 490.2046975656187, 475.43665048506546, 450.50989299543585, 420.6035281428809, 402.35682913674157, 395.7445517955127, 380.5324726923594], 'mean_squared_logarithmic_error': [3.4728754542720313, 0.4348138111515022, 0.4269675411510403, 0.41624336676188084, 0.4010561436988285, 0.3865401439532164, 0.3783819410077019, 0.37229918515822924, 0.3583430062584886, 0.3575915522988093, 0.34932087194219463, 0.33919731242183077, 0.3319113465872, 0.3299941059741716, 0.3241569667666879]}\n",
    "\n",
    "Train error is: 23.154237108683564\n",
    "Validation error is: 33.01510854257875"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_hist.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train error is:', np.sqrt(mean_squared_error(y_train,model.predict(X_train))))\n",
    "\n",
    "print('Validation error is:', np.sqrt(mean_squared_error(y_val,model.predict(X_val))))\n",
    "\n",
    "print('Test error is:', np.sqrt(mean_squared_error(y_test,model.predict(scaler.transform(X_test)))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
